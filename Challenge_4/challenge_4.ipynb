{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function for generating array of Similar (Real) Pairs from given dataset and reducing image size:\n",
    "\n",
    "def similar_pairs(LIST,dataset):\n",
    "    \n",
    "    X =[]\n",
    "    y =[]\n",
    "    \n",
    "    for number in LIST:\n",
    "        print(number)\n",
    "        \n",
    "        location = 'Signatures_2/' + dataset + '/'\n",
    "        \n",
    "        # Removing '.DS_Store' from list:\n",
    "        if '.DS_Store' in os.listdir(location + number + '/Real/'):\n",
    "            listR = os.listdir(location + number + '/Real/')\n",
    "            listR.remove('.DS_Store')\n",
    "        else:\n",
    "            listR = os.listdir(location + number + '/Real/')\n",
    "        \n",
    "        for pairs in combinations(listR,2):\n",
    "            \n",
    "            image1 = np.asarray(Image.open(location + number +'/Real/'+ pairs[0]).convert('RGB').resize((100,100)))\n",
    "            image2 = np.asarray(Image.open(location + number +'/Real/'+ pairs[1]).convert('RGB').resize((100,100)))\n",
    "            \n",
    "            \n",
    "            X.append([image1,image2])\n",
    "            y.append(float(1))\n",
    "            \n",
    "        \n",
    "    return np.array(X) / 255 , np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dissimilar_pair(LIST,dataset):\n",
    "    \n",
    "    X =[]\n",
    "    y =[]\n",
    "    \n",
    "    for number in LIST:\n",
    "        print(number)\n",
    "        location = 'Signatures_2/' + dataset + '/'\n",
    "        R = location + number + '/Real/'\n",
    "        F = location + number + '/Forged/'\n",
    "        \n",
    "        # Removing '.DS_Store' from lists:\n",
    "        list1=os.listdir(R)\n",
    "        if '.DS_Store' in list1:\n",
    "            list1.remove('.DS_Store')\n",
    "        \n",
    "        list2=os.listdir(F)\n",
    "        if '.DS_Store' in list2:\n",
    "            list2.remove('.DS_Store')\n",
    "        \n",
    "        if '.DS_Store' in list2:\n",
    "            list2 = list2[1:]\n",
    "            if '.DS_Store' in list1:\n",
    "                list1.remove('.DS_Store')\n",
    "        else:\n",
    "            list1 = list1[1:]\n",
    "            if '.DS_Store' in list2:\n",
    "                list2.remove('.DS_Store')\n",
    "            \n",
    "        # Pairing Dissimilar Pairs\n",
    "        output = [[a, b] for a in list1  \n",
    "          for b in list2 if a != b]\n",
    "        \n",
    "        for pairs in output:\n",
    "            \n",
    "            image1 = np.asarray(Image.open(location + number +'/Real/'+ pairs[0]).convert('RGB').resize((100,100)))\n",
    "            image2 = np.asarray(Image.open(location + number +'/Forged/'+ pairs[1]).convert('RGB').resize((100,100)))\n",
    "            \n",
    "            \n",
    "            X.append([image1,image2])\n",
    "            y.append(float(0))                        \n",
    "        \n",
    "    return np.array(X) / 255 , np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Lists of Training and Testing Set Input Folders:\n",
    "training_set = os.listdir('./Signatures_2/Train_set')\n",
    "testing_set = os.listdir('./Signatures_2/Test_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting '.DS_Store' files from the lists:\n",
    "training_set.remove('.DS_Store')\n",
    "testing_set.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Similar pairs of Real Signature from the Training Set\n",
    "train_sp, train_sp_label = similar_pairs(training_set,'Train_set')\n",
    "# np.save('Train_sp_100.npy', train_sp)\n",
    "# np.save('Train_sp_label.npy', train_sp_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Similar pairs of Real Signature from the Testing Set\n",
    "test_sp, test_sp_label = similar_pairs(testing_set,'Test_set')\n",
    "# np.save('Test_sp_100.npy', test_sp)\n",
    "# np.save('Test_sp_label.npy', test_sp_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Dissimilar pairs of Real and Forged Signatures from the Training Set\n",
    "train_fp, train_fp_label = dissimilar_pair(training_set,'Train_set')\n",
    "# np.save('Train_fp_100.npy', train_fp)\n",
    "# np.save('Train_fp_label.npy', train_fp_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Dissimilar pairs of Real and Forged Signatures from the Testing Set\n",
    "test_fp, test_fp_label = dissimilar_pair(testing_set,'Test_set')\n",
    "# np.save('Test_fp_100.npy', test_fp)\n",
    "# np.save('Test_fp_label.npy', test_fp_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sp = np.load('Train_sp_100.npy')\n",
    "# train_sp_label = np.load('Train_sp_label.npy')\n",
    "# train_fp = np.load('Train_fp_100.npy')\n",
    "# train_fp_label = np.load('Train_fp_label.npy')\n",
    "# test_sp = np.load('Test_sp_100.npy')\n",
    "# test_sp_label = np.load('Test_sp_label.npy')\n",
    "# test_fp =  np.load('Test_fp_100.npy')\n",
    "# test_fp_label = np.load('Test_fp_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Similar Training Pairs:',train_sp.shape[0], '\\n')\n",
    "print('Dissimilar Training Pairs:',train_fp.shape[0] , '\\n')\n",
    "print('Similar Testing Pairs:',test_sp.shape[0] , '\\n')\n",
    "print('Dissimilar Testing Pairs:',test_fp.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Similar and Dissimilar pairs of Training and Testing Sets, respectively and shuffle the rows:\n",
    "Training_pairs = np.vstack((train_sp, train_fp ))\n",
    "Testing_pairs = np.vstack((test_sp, test_fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Training_pairs.npy', Training_pairs)\n",
    "np.save('Testing_pairs.npy', Testing_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training_pairs = np.load('Training_pairs.npy')\n",
    "# Testing_pairs = np.load('Testing_pairs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_labels = np.hstack((train_sp_label, train_fp_label))\n",
    "Testing_labels = np.hstack((test_sp_label, test_fp_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.get_state()\n",
    "np.random.shuffle(Training_pairs)\n",
    "np.random.set_state(random_state)\n",
    "np.random.shuffle(Training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.get_state()\n",
    "np.random.shuffle(Testing_pairs)\n",
    "np.random.set_state(random_state)\n",
    "np.random.shuffle(Testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten, AveragePooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import initializers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (100,100,3)\n",
    "left_input= Input((100,100,3))\n",
    "right_input= Input((100,100,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Siamese Neural Network:\n",
    "\n",
    "def get_siamese_model(left_input, right_input):\n",
    "    \n",
    "    # Setting up Parameters for Neural Network:\n",
    "    initialize_weights = initializers.RandomNormal(mean = 0.5 ,stddev=0.01)\n",
    "    initialize_bias = initializers.RandomNormal(mean = 0.5 ,stddev=0.01)\n",
    "        \n",
    "    # Convolutional Neural Network:\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(100, (3,3), strides= 1, padding = \"same\", activation='relu', \n",
    "                     input_shape=input_shape, kernel_initializer=initialize_weights, \n",
    "                     kernel_regularizer=l2(2e-4)))\n",
    "    model.add(AveragePooling2D(2,2))\n",
    "        \n",
    "    model.add(Conv2D(200, (3,3), strides= 1, padding = \"same\", activation='relu', \n",
    "                     kernel_initializer=initialize_weights, bias_initializer=initialize_bias, \n",
    "                     kernel_regularizer=l2(2e-4)))\n",
    "    model.add(AveragePooling2D(2,2))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(400, activation='sigmoid', kernel_initializer=initialize_weights, \n",
    "                     bias_initializer=initialize_bias,kernel_regularizer=l2(1e-3)))\n",
    "    \n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = Dense(1,activation='sigmoid',bias_initializer=initialize_bias)(L1_distance)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_siamese_model(left_input, right_input)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Adam optimizer and compiling the model with loss function and accuracy:\n",
    "\n",
    "optimizer = Adam(lr = 0.001)\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer, metrics = ['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying KFold Validation on Training Set:\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits = 3)\n",
    "c=1\n",
    "for train_index, test_index in kfold.split(Training_pairs, Training_labels):\n",
    "    \n",
    "    print('Fold:',c)\n",
    "    c+=1\n",
    "    \n",
    "    X_train = Training_pairs[train_index]\n",
    "    X_test = Training_pairs[test_index]\n",
    "    y_train = Training_labels[train_index]\n",
    "    y_test = Training_labels[test_index]\n",
    "    \n",
    "    model.fit( [X_train[:,0],X_train[:,1]] , y_train, validation_data = ([X_test[:,0],X_test[:,1]] , y_test ), \n",
    "              batch_size=30, epochs=2, workers=-1 )\n",
    "    print('\\n-------------------------------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model on the complete Training set:\n",
    "\n",
    "model.fit( [Training_pairs[:,0], Training_pairs[:,1]] , Training_labels, \n",
    "              batch_size=30, epochs=2, workers=-1, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model with Testing Set:\n",
    "\n",
    "model.evaluate([Testing_pairs[:,0],Testing_pairs[:,1]], Testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification of Signatures from Testing Set:\n",
    "\n",
    "predict = model.predict([Testing_pairs[:,0], Testing_pairs[:,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing performance metrics of the prediction:\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(confusion_matrix(Testing_labels, predict.round()))\n",
    "print(classification_report(Testing_labels, predict.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Siamese Model predicts with a very high Accuracy for the classification of Similar and Dissimilar Signatures.\n",
    "# This could be a result of overfitting the data with 5 folds and again training over the complete dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
